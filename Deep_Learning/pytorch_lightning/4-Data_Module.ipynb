{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSE(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"sum_squared_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        sum_squared_error = torch.sum((preds - target) ** 2)\n",
    "        total = target.numel()\n",
    "        self.sum_squared_error += sum_squared_error\n",
    "        self.total += total\n",
    "\n",
    "    def compute(self):\n",
    "        return self.sum_squared_error / self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\",num_classes=num_classes)\n",
    "        self.f1_score = torchmetrics.F1Score(task=\"multiclass\",num_classes=num_classes)\n",
    "\n",
    "        self.custom_mse = CustomMSE()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "\n",
    "        accuracy = self.accuracy(scores,y)\n",
    "        f1_score = self.f1_score(scores,y)\n",
    "\n",
    "        accuracy = self.custom_mse(scores, y) # for custom accuracy\n",
    "\n",
    "        # on_step=False: This means the metrics will not be logged at every training step.\n",
    "        # on_epoch=True: This means the metrics will be logged at the end of each training epoch.\n",
    "\n",
    "        # Epoch: A full pass through the entire dataset, encompassing all batches.\n",
    "        # Step: A single update of model parameters after processing one batch of data.\n",
    "\n",
    "        self.log_dict({'train_loss': loss, 'train_accuracy': accuracy, 'train_f1_score': f1_score},\n",
    "              on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        return loss, scores, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MyDataModule(batch_size=64, data_dir='./data',num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2 gpus and train for minimum of 3 epochs\n",
    "# precision=16: This enables training with mixed precision, \n",
    "# specifically 16-bit floating point (half precision). \n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",devices=2,min_epochs=3,precision=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Just pass data_module object and trainer automatically recognizes what split to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit: Trains the model using the training data and evaluates it periodically on the validation data.\n",
    "# validate: Evaluates the model on the validation data after or outside of the training loop.\n",
    "# test: Evaluates the model on the test data to assess its performance on new, unseen data.\n",
    "\n",
    "trainer.fit(model,data_module)\n",
    "trainer.validate(model,data_module)\n",
    "trainer.test(model,data_module)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
