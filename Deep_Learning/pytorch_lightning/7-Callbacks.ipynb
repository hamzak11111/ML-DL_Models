{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Callbacks in PyTorch Lightning are a way to execute custom code at specific points during the training and validation process. They provide a flexible mechanism to perform actions such as logging metrics, saving model checkpoints, adjusting learning rates, or early stopping based on certain conditions.\n",
    "\n",
    "##### Hereâ€™s a quick rundown of common callbacks in PyTorch Lightning:\n",
    "\n",
    "##### ModelCheckpoint: Saves the model at specified intervals or when certain metrics improve. Useful for keeping the best version of the model.\n",
    "\n",
    "##### EarlyStopping: Stops training early if a monitored metric does not improve for a certain number of epochs, preventing overfitting and saving computation.\n",
    "\n",
    "##### LearningRateMonitor: Logs learning rate changes during training, which can help in visualizing and debugging the learning rate schedule.\n",
    "\n",
    "##### ProgressBar: Displays a progress bar during training and validation, which helps in monitoring the training process visually.\n",
    "\n",
    "##### TensorBoard: Integrates with TensorBoard to log metrics, model graphs, and other useful visualizations.\n",
    "\n",
    "##### You can also create custom callbacks by subclassing pl.Callback and overriding methods like on_epoch_end, on_train_batch_end, or on_validation_end to insert your custom logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class CustomModelCheckpoint(pl.Callback):\n",
    "    def __init__(self, monitor='val_loss', mode='min', save_top_k=1):\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_top_k = save_top_k\n",
    "        self.best_score = float('inf') if mode == 'min' else -float('inf')\n",
    "        self.saved_models = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        current_score = trainer.callback_metrics.get(self.monitor)\n",
    "        if current_score is not None:\n",
    "            if (self.mode == 'min' and current_score < self.best_score) or (self.mode == 'max' and current_score > self.best_score):\n",
    "                self.best_score = current_score\n",
    "                # Save model checkpoint\n",
    "                checkpoint_path = f\"model_checkpoint_epoch_{trainer.current_epoch}.ckpt\"\n",
    "                trainer.save_checkpoint(checkpoint_path)\n",
    "                self.saved_models.append(checkpoint_path)\n",
    "                # Keep only the top_k models\n",
    "                if len(self.saved_models) > self.save_top_k:\n",
    "                    oldest_model = self.saved_models.pop(0)\n",
    "                    os.remove(oldest_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class CustomEarlyStopping(pl.Callback):\n",
    "    def __init__(self, monitor='val_loss', patience=3, mode='min'):\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.best_score = float('inf') if mode == 'min' else -float('inf')\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        current_score = trainer.callback_metrics.get(self.monitor)\n",
    "        if current_score is not None:\n",
    "            if (self.mode == 'min' and current_score < self.best_score) or (self.mode == 'max' and current_score > self.best_score):\n",
    "                self.best_score = current_score\n",
    "                self.wait = 0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    trainer.should_stop = True\n",
    "                    self.stopped_epoch = trainer.current_epoch\n",
    "                    print(f\"Early stopping at epoch {self.stopped_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LearningRateMonitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class CustomLearningRateMonitor(pl.Callback):\n",
    "    def on_batch_end(self, trainer, pl_module):\n",
    "        for i, param_group in enumerate(trainer.optimizers[0].param_groups):\n",
    "            lr = param_group['lr']\n",
    "            print(f\"Epoch: {trainer.current_epoch}, Batch: {trainer.global_step}, Learning Rate: {lr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ProgressBar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class CustomProgressBar(pl.Callback):\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        print(f\"Training started for {trainer.max_epochs} epochs.\")\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Epoch {trainer.current_epoch} ended.\")\n",
    "    \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class CustomTensorBoard(pl.Callback):\n",
    "    def __init__(self, log_dir='logs'):\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        for name, param in pl_module.named_parameters():\n",
    "            self.writer.add_histogram(name, param, trainer.current_epoch)\n",
    "        self.writer.flush()\n",
    "    \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        self.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(callbacks=[CustomModelCheckpoint(), CustomEarlyStopping(), CustomLearningRateMonitor(), CustomProgressBar(), CustomTensorBoard()])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
