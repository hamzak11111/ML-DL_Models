{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning is a high-level library built on top of PyTorch designed to simplify the process of writing, training, and scaling PyTorch code. It abstracts much of the boilerplate code typically associated with PyTorch, allowing developers to focus more on the research and development of their models rather than on the engineering and repetitive coding tasks. Here are the key reasons why it should be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Features and Benefits:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifies Code Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning organizes your PyTorch code into a modular and standardized format. This separation of concerns helps in maintaining a cleaner and more readable codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handles Boilerplate Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It takes care of the training loop, validation loop, and other repetitive tasks, letting you concentrate on writing the core logic of your models and experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easier Experimentation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning allows for easy configuration and modification of experiments. You can change hyperparameters, models, and other components without changing much of the surrounding code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using PyTorch Lightning, you inherently adopt many best practices in model training, validation, logging, and checkpointing, which can lead to more robust and reliable experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and Checkpointing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It integrates seamlessly with logging frameworks like TensorBoard, MLFlow, and Weights & Biases, facilitating better experiment tracking and visualization. It also handles model checkpointing automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging and Profiling:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning includes tools for debugging and profiling your code, helping you identify bottlenecks and optimize performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Components of PyTorch Lightning:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightningModule:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the core class where you define your model architecture, forward pass, training step, validation step, and test step. It encapsulates all the necessary parts for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trainer class handles the entire training loop. It abstracts the complexity of training, validation, and testing, and can be configured with various settings like the number of epochs, GPU/TPU usage, distributed training, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are classes that encapsulate all data loading logic, making it easy to manage data pipelines and pre-processing steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(LitModel, self).__init__()\n",
    "        self.layer_1 = nn.Linear(28 * 28, 128)\n",
    "        self.layer_2 = nn.Linear(128, 256)\n",
    "        self.layer_3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.layer_3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.nll_loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "# Data\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = MNIST('', train=True, download=True, transform=transform)\n",
    "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "train_loader = DataLoader(mnist_train, batch_size=64)\n",
    "val_loader = DataLoader(mnist_val, batch_size=64)\n",
    "\n",
    "# Training\n",
    "model = LitModel()\n",
    "trainer = pl.Trainer(max_epochs=5)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
