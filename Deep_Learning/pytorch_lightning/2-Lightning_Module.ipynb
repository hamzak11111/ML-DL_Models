{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the pytorch-lightning framework, certain method names in a LightningModule class\n",
    "#  are reserved and have specific purposes. \n",
    "# These methods are not Python keywords,\n",
    "#  but rather special methods that pytorch-lightning expects and uses to manage the \n",
    "# training, validation, and testing loops. Here's a brief overview of these methods:\n",
    "\n",
    "# __init__: Initializes the module. T\n",
    "# his is a standard Python method used to initialize the instance attributes.\n",
    "\n",
    "# forward: Defines the forward pass of the neural network. \n",
    "# This is a standard method in PyTorch models, \n",
    "# specifying how the input data flows through the network layers.\n",
    "\n",
    "# training_step: Defines a single step of the training loop. \n",
    "# pytorch-lightning uses this method to know what operations to perform during training \n",
    "# for each batch of data.\n",
    "\n",
    "# validation_step: Defines a single step of the validation loop. \n",
    "# Similar to training_step, but for the validation phase.\n",
    "\n",
    "# test_step: Defines a single step of the testing loop. \n",
    "# Similar to training_step, but for the testing phase.\n",
    "\n",
    "# _common_step: This is a custom method you've defined to avoid code duplication between \n",
    "# training_step, validation_step, and test_step. \n",
    "# It is not a special method recognized by pytorch-lightning, \n",
    "# but rather a helper method to reduce redundancy in your code.\n",
    "\n",
    "# predict_step: Defines a single step for prediction. \n",
    "# pytorch-lightning uses this method to perform predictions on new data.\n",
    "\n",
    "# configure_optimizers: Specifies the optimizer(s) to use during training. \n",
    "# pytorch-lightning uses this method to configure the optimizer(s) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        return loss, scores, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MyDataModule(batch_size=64, data_dir='./data',num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2 gpus and train for minimum of 3 epochs\n",
    "# precision=16: This enables training with mixed precision, \n",
    "# specifically 16-bit floating point (half precision). \n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",devices=2,min_epochs=3,precision=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just pass data_module object and trainer automatically recognizes what split to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit: Trains the model using the training data and evaluates it periodically on the validation data.\n",
    "# validate: Evaluates the model on the validation data after or outside of the training loop.\n",
    "# test: Evaluates the model on the test data to assess its performance on new, unseen data.\n",
    "\n",
    "trainer.fit(model,train_loader,val_loader)\n",
    "trainer.validate(model,val_loader)\n",
    "trainer.test(model,test_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
